{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradyumna4998/Intro-to-ML/blob/main/HW7_Q2_Intro_to_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Introduction to Machine Learning\n",
        "\n",
        "                                            Homework 7\n",
        "\n",
        "Question 2\n",
        "\n",
        "Name : V.pradyumna\n",
        "\n",
        "Student ID : 801345963"
      ],
      "metadata": {
        "id": "oJjL3XdB63hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LsajevC6-1p",
        "outputId": "9b67328a-3e03-4044-d195-8d0be82106af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.2\n",
            "time: 350 µs (started: 2024-12-04 20:00:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAnWygYL7LD9",
        "outputId": "ed36ba46-1ee3-43e4-e9f9-ff7941cdaaf9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 25.5 s (started: 2024-12-04 20:00:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYbBRWWi7RWY",
        "outputId": "ffc4de3d-e8c3-4853-a138-fee48b7eef75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 51.7 ms (started: 2024-12-04 20:00:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spwr8VXm7c5u",
        "outputId": "3f622419-cb0e-449a-8fdf-d431b66553eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 505 µs (started: 2024-12-04 20:00:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and standard(std)\n",
        "images = torch.stack([img_t for img_t, _ in training_dataset], dim=3)\n",
        "mean = images.view(3, -1).mean(dim=1)\n",
        "std = images.view(3, -1).std(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlkSew-B7iNw",
        "outputId": "0713268b-e010-4966-9cef-ae87e987cbbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 34.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "time: 27.9 s (started: 2024-12-04 20:00:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCEKj_i57kiB",
        "outputId": "82e06d4f-e936-4763-8a81-49667573a5ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4914, 0.4822, 0.4465])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 37.6 ms (started: 2024-12-04 20:01:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RDU-VcB7pWX",
        "outputId": "ae8ec145-3726-43fa-dcdc-08090e9e58f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2470, 0.2435, 0.2616])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.84 ms (started: 2024-12-04 20:01:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = datasets.CIFAR10(\n",
        "    './data', train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFUfmnC37q_i",
        "outputId": "6e6f0afd-9fff-4684-be8b-af4e14cff682"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 474 ms (started: 2024-12-04 20:01:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "     './data', train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWWnJ1dK7uQp",
        "outputId": "7f4f1276-5cce-4f20-97be-f6c609e2fbd7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 358 ms (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_loader = DataLoader(cifar10, batch_size=32, shuffle=True, num_workers=2)\n",
        "testing_loader = DataLoader(cifar10_val, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CnSGBvt7xJK",
        "outputId": "04db7dc0-352e-486c-cffb-58f4af9f2545"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 975 µs (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH74OLFa71ed",
        "outputId": "d2968110-dcf1-4d33-d6ac-275aa18754eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 420 µs (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCb-Te-p75h0",
        "outputId": "10139218-c1ff-4d35-a1bb-ee294b21e8bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "time: 411 µs (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import datetime\n",
        "def training_loop(epochs, optimizer, model, loss_function, training_loader):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        training_loss = 0.0\n",
        "        for images, labels in training_loader:\n",
        "            images = images.to(device=device)  # <1>\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            training_loss += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 2 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                training_loss / len(training_loader)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ivm1AH878F2",
        "outputId": "c6a5f928-29e4-4ea3-ca79-199adfdce5d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 565 µs (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual_Block(nn.Module):\n",
        "    def __init__(self, n_chans):\n",
        "        super(Residual_Block, self).__init__()\n",
        "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
        "                              padding=1, bias=False)  # <1>\n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
        "                                      nonlinearity='relu')  # <2>\n",
        "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.batch_norm(out)\n",
        "        out = torch.relu(out)\n",
        "        return out + x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40RrByFm8Ad5",
        "outputId": "49b99b3a-5414-4c6d-c5a9-d6af004ec7d0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 575 µs (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "class NetResDeep(nn.Module):\n",
        "    def __init__(self, n_chans1=32, n_blocks=10):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *(n_blocks * [Residual_Block(n_chans=n_chans1)]))\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = self.resblocks(out)\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGpyEzK88EQb",
        "outputId": "ece21560-42fb-43f7-b1b1-a0ab1108cac6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 833 µs (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCj-O_BJ8KKl",
        "outputId": "d3583674-bc69-46f9-d1dc-d8d68c4565bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.4 ms (started: 2024-12-04 20:01:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "training_loop(\n",
        "    epochs = 200,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_function = loss_fn,\n",
        "    training_loader = training_loader,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4R0p8Eo8My-",
        "outputId": "3d2d17e4-cd2d-4c7f-fedd-6b0df0e4e36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-04 20:01:59.241117 Epoch 1, Training loss 1.581621990627916\n",
            "2024-12-04 20:02:31.600274 Epoch 2, Training loss 1.2795808721031048\n",
            "2024-12-04 20:03:37.986840 Epoch 4, Training loss 1.0715397235604334\n",
            "2024-12-04 20:04:42.162634 Epoch 6, Training loss 0.9441771852382848\n",
            "2024-12-04 20:05:48.066993 Epoch 8, Training loss 0.8640034083021961\n",
            "2024-12-04 20:06:52.118347 Epoch 10, Training loss 0.8036986036858952\n",
            "2024-12-04 20:07:58.173224 Epoch 12, Training loss 0.752829780924877\n",
            "2024-12-04 20:09:03.859696 Epoch 14, Training loss 0.7067300276979757\n",
            "2024-12-04 20:10:08.526991 Epoch 16, Training loss 0.66741770820517\n",
            "2024-12-04 20:11:14.616550 Epoch 18, Training loss 0.6336688590942098\n",
            "2024-12-04 20:12:19.349393 Epoch 20, Training loss 0.5936948609367366\n",
            "2024-12-04 20:13:25.009533 Epoch 22, Training loss 0.5680613198954557\n",
            "2024-12-04 20:14:30.859550 Epoch 24, Training loss 0.5395997961362203\n",
            "2024-12-04 20:15:35.072069 Epoch 26, Training loss 0.5091024690372625\n",
            "2024-12-04 20:16:40.801647 Epoch 28, Training loss 0.48252104234691623\n",
            "2024-12-04 20:17:45.982916 Epoch 30, Training loss 0.4536788569035167\n",
            "2024-12-04 20:18:50.848457 Epoch 32, Training loss 0.43376682320715754\n",
            "2024-12-04 20:19:56.921565 Epoch 34, Training loss 0.40638418916090896\n",
            "2024-12-04 20:21:01.927375 Epoch 36, Training loss 0.38389495716824107\n",
            "2024-12-04 20:22:08.073261 Epoch 38, Training loss 0.3650396318196945\n",
            "2024-12-04 20:23:14.412350 Epoch 40, Training loss 0.3441150712799126\n",
            "2024-12-04 20:24:19.472960 Epoch 42, Training loss 0.3248765015532516\n",
            "2024-12-04 20:25:25.345579 Epoch 44, Training loss 0.30739042479652134\n",
            "2024-12-04 20:26:30.796141 Epoch 46, Training loss 0.28725077093639056\n",
            "2024-12-04 20:27:35.121240 Epoch 48, Training loss 0.2764916315755818\n",
            "2024-12-04 20:28:41.108216 Epoch 50, Training loss 0.2616087335304274\n",
            "2024-12-04 20:29:46.741654 Epoch 52, Training loss 0.24762360648307485\n",
            "2024-12-04 20:30:51.800659 Epoch 54, Training loss 0.2376338310256095\n",
            "2024-12-04 20:31:57.604722 Epoch 56, Training loss 0.22673646142373788\n",
            "2024-12-04 20:33:01.863069 Epoch 58, Training loss 0.21193644919707388\n",
            "2024-12-04 20:34:07.273588 Epoch 60, Training loss 0.2048749930695443\n",
            "2024-12-04 20:35:13.142889 Epoch 62, Training loss 0.19955873091050297\n",
            "2024-12-04 20:36:17.875148 Epoch 64, Training loss 0.1861114198042846\n",
            "2024-12-04 20:37:23.926917 Epoch 66, Training loss 0.18475032047864934\n",
            "2024-12-04 20:38:28.904772 Epoch 68, Training loss 0.160586891030882\n",
            "2024-12-04 20:39:35.049936 Epoch 70, Training loss 0.16608550826204382\n",
            "2024-12-04 20:40:41.036241 Epoch 72, Training loss 0.16395425612046738\n",
            "2024-12-04 20:41:45.347926 Epoch 74, Training loss 0.1441231604185258\n",
            "2024-12-04 20:42:50.686599 Epoch 76, Training loss 0.14352760106767273\n",
            "2024-12-04 20:43:56.424847 Epoch 78, Training loss 0.1445563512235184\n",
            "2024-12-04 20:45:01.388968 Epoch 80, Training loss 0.13236535102026495\n",
            "2024-12-04 20:46:06.915941 Epoch 82, Training loss 0.13419985652574107\n",
            "2024-12-04 20:47:11.694428 Epoch 84, Training loss 0.11418955280431811\n",
            "2024-12-04 20:48:16.212581 Epoch 86, Training loss 0.11103280801302202\n",
            "2024-12-04 20:49:21.584619 Epoch 88, Training loss 0.11091454874608554\n",
            "2024-12-04 20:50:25.060288 Epoch 90, Training loss 0.11966557992128887\n",
            "2024-12-04 20:51:29.448260 Epoch 92, Training loss 0.10546753694653697\n",
            "2024-12-04 20:52:34.218917 Epoch 94, Training loss 0.10818221321584501\n",
            "2024-12-04 20:53:38.639993 Epoch 96, Training loss 0.10749450464383177\n",
            "2024-12-04 20:54:44.399174 Epoch 98, Training loss 0.10683496938410596\n",
            "2024-12-04 20:55:49.279656 Epoch 100, Training loss 0.08711139382901753\n",
            "2024-12-04 20:56:54.681043 Epoch 102, Training loss 0.08752649497937635\n",
            "2024-12-04 20:58:00.860040 Epoch 104, Training loss 0.09164392180174473\n",
            "2024-12-04 20:59:04.615277 Epoch 106, Training loss 0.08877697452811628\n",
            "2024-12-04 21:00:10.283653 Epoch 108, Training loss 0.09148461504200901\n",
            "2024-12-04 21:01:14.630931 Epoch 110, Training loss 0.07732914985957187\n",
            "2024-12-04 21:02:17.693375 Epoch 112, Training loss 0.07257128472618553\n",
            "2024-12-04 21:03:22.569717 Epoch 114, Training loss 0.09059301383363742\n",
            "2024-12-04 21:04:26.845775 Epoch 116, Training loss 0.08470703004473572\n",
            "2024-12-04 21:05:30.757893 Epoch 118, Training loss 0.08599137592529228\n",
            "2024-12-04 21:06:35.948760 Epoch 120, Training loss 0.07802178642428906\n",
            "2024-12-04 21:07:39.511925 Epoch 122, Training loss 0.06662873456173267\n",
            "2024-12-04 21:08:44.450851 Epoch 124, Training loss 0.07333269494812714\n",
            "2024-12-04 21:09:48.761444 Epoch 126, Training loss 0.056508678712285355\n",
            "2024-12-04 21:10:51.812574 Epoch 128, Training loss 0.07609360211755858\n",
            "2024-12-04 21:11:56.412053 Epoch 130, Training loss 0.06414815129593648\n",
            "2024-12-04 21:13:00.675925 Epoch 132, Training loss 0.06831233347993644\n",
            "2024-12-04 21:14:03.919935 Epoch 134, Training loss 0.061540035079713014\n",
            "2024-12-04 21:15:08.098195 Epoch 136, Training loss 0.06601464842604125\n",
            "2024-12-04 21:16:12.683092 Epoch 138, Training loss 0.06828368310414011\n",
            "2024-12-04 21:17:16.862616 Epoch 140, Training loss 0.0551639093172597\n",
            "2024-12-04 21:18:21.804320 Epoch 142, Training loss 0.05988591909702773\n",
            "2024-12-04 21:19:25.644684 Epoch 144, Training loss 0.0679651891544696\n",
            "2024-12-04 21:20:30.537474 Epoch 146, Training loss 0.05256339806877658\n",
            "2024-12-04 21:21:35.291844 Epoch 148, Training loss 0.045016354696511725\n",
            "2024-12-04 21:22:38.161085 Epoch 150, Training loss 0.05272024017413071\n",
            "2024-12-04 21:23:42.102626 Epoch 152, Training loss 0.06195227174812398\n",
            "2024-12-04 21:24:45.758615 Epoch 154, Training loss 0.054948992488538\n",
            "2024-12-04 21:25:48.914743 Epoch 156, Training loss 0.06098282075725345\n",
            "2024-12-04 21:26:53.632402 Epoch 158, Training loss 0.04844156405439558\n",
            "2024-12-04 21:27:57.138291 Epoch 160, Training loss 0.05216118977937118\n",
            "2024-12-04 21:29:01.395960 Epoch 162, Training loss 0.06822866206168535\n",
            "2024-12-04 21:30:05.658794 Epoch 164, Training loss 0.054078840272547585\n",
            "2024-12-04 21:31:09.028841 Epoch 166, Training loss 0.03936984891088525\n",
            "2024-12-04 21:32:14.838112 Epoch 168, Training loss 0.041586663261798454\n",
            "2024-12-04 21:33:18.855674 Epoch 170, Training loss 0.03465862107890223\n",
            "2024-12-04 21:34:21.614242 Epoch 172, Training loss 0.042700984875733164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "def validate(model, train_loader, val_loader):\n",
        "    acc_dict = {}\n",
        "    predictions = []\n",
        "    exp_labels = []\n",
        "\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)\n",
        "                labels = labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "                predictions.extend(predicted.cpu().numpy())\n",
        "                exp_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "        acc_dict[name] = correct / total\n",
        "    return acc_dict, predictions, exp_labels"
      ],
      "metadata": {
        "id": "PIJFBOEjgyQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, predictions, expected_labels = validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "zCBagltmhCOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions, expected_labels, target_names=class_names))"
      ],
      "metadata": {
        "id": "Gb-PpnGyhPkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(predictions, expected_labels, average='macro')\n",
        "recall = recall_score(predictions, expected_labels, average='macro')\n",
        "cnf_matrix = confusion_matrix(predictions, expected_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cnf_matrix, annot=True, cmap=\"viridis\", fmt='g',\n",
        "            xticklabels = class_names, yticklabels = class_names)\n",
        "plt.title('Resnet10 Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SNpZeqyvhQ6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetWidth(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "aDZHTA8xhhpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
        "                        train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            l2_lambda = 0.001\n",
        "            l2_norm = sum(p.pow(2.0).sum()\n",
        "                          for p in model.parameters())  # <1>\n",
        "            loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "        if epoch == 1 or epoch % 2 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))\n"
      ],
      "metadata": {
        "id": "6GF0I7eyhnqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_wd = NetWidth(n_chans1=32).to(device=device)\n",
        "optimizer_wd = optim.SGD(model_wd.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "training_loop_l2reg(\n",
        "    n_epochs = 200,\n",
        "    optimizer = optimizer_wd,\n",
        "    model = model_wd,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = training_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "G3_zMzLOhrOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "accuracy_weight_delay, predictions_weight_delay, expected_labels_weight_delay = validate(model_wd, train_loader, val_loader)\n"
      ],
      "metadata": {
        "id": "SD0TXsdWx6T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions_weight_delay, expected_labels_weight_delay, target_names=class_names))"
      ],
      "metadata": {
        "id": "Pyz5MY29x1Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_weight_delay = precision_score(predictions_weight_delay, expected_labels_weight_delay, average='macro')\n",
        "recall_weight_delay = recall_score(predictions_weight_delay, expected_labels_weight_delay, average='macro')\n",
        "cnf_matrix_weight_delay = confusion_matrix(predictions_weight_delay, expected_labels_weight_delay)\n"
      ],
      "metadata": {
        "id": "ArDoUzs1x2Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cnf_matrix_weight_delay, annot=True, cmap=\"viridis\", fmt='g',\n",
        "            xticklabels = class_names, yticklabels = class_names)\n",
        "plt.title('Resnet10 Confusion Matrix(Weight decay = 0.01)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UPlftP_fyBfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.3)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = self.conv1_dropout(out)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = self.conv2_dropout(out)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "e7SR0uoUyOEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dropout = NetDropout(n_chans1=32).to(device=device)\n",
        "optimizer_dropout = optim.SGD(model_dropout.parameters(), lr=1e-2)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    epochs = 300,\n",
        "    optimizer = optimizer_dropout,\n",
        "    model = model_dropout,\n",
        "    loss_function = loss_function,\n",
        "    training_loader = training_loader,\n",
        ")\n"
      ],
      "metadata": {
        "id": "IDi6bNh9ySa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dropout, predictions_dropout, expected_labels_dropout = validate(model_dropout, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "XCIm2Xo_yWLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions_dropout, expected_labels_dropout, target_names=class_names))\n"
      ],
      "metadata": {
        "id": "2oFceYjqyoxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_dropout = precision_score(predictions_dropout, expected_labels_dropout, average='macro')\n",
        "recall_dropout = recall_score(predictions_dropout, expected_labels_dropout, average='macro')\n",
        "cnf_matrix_dropout = confusion_matrix(predictions_dropout, expected_labels_dropout)"
      ],
      "metadata": {
        "id": "MbRuwF8Cyr38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cnf_matrix_dropout, annot=True, cmap=\"viridis\", fmt='g',\n",
        "            xticklabels = class_names, yticklabels = class_names)\n",
        "plt.title('Resnet10  Confusion Matrix(dropout with p = 0.3)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WplW_jj-yv5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetBatchNorm(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1_batchnorm(self.conv1(x))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = self.conv2_batchnorm(self.conv2(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ijQ0_er_yz25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_batch_normalization = NetBatchNorm(n_chans1=32).to(device=device)\n",
        "optimizer_batch_normalization = optim.SGD(model_batch_normalization.parameters(), lr=1e-2)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    epochs = 300,\n",
        "    optimizer = optimizer_batch_normalization,\n",
        "    model = model_batch_normalization,\n",
        "    loss_function = loss_function,\n",
        "    training_loader = training_loader,\n",
        ")"
      ],
      "metadata": {
        "id": "5XesHas_zYVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_batch_normalization, predictions_batch_normalization, expected_labels_batch_normalization = validate(model_dropout, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "aECKrq2tzadx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions_dropout, expected_labels_dropout, target_names=class_names))"
      ],
      "metadata": {
        "id": "LRJATaSizrnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_batch_norm = precision_score(predictions_batch_normalization, expected_labels_batch_normalization, average='macro')\n",
        "recall_batch_norm = recall_score(predictions_batch_normalization, expected_labels_batch_normalization, average='macro')\n",
        "cnf_matrix_batch_norm = confusion_matrix(predictions_batch_normalization, expected_labels_batch_normalization)"
      ],
      "metadata": {
        "id": "xj1cGlXbzjaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cnf_matrix_batch_norm, annot=True, cmap=\"viridis\", fmt='g',\n",
        "            xticklabels = class_names, yticklabels = class_names)\n",
        "plt.title('Resnet10 Confusion Matrix(Batch Normalization)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NspF3o5czok0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}